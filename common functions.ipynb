{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eecf83ec-ff80-4f0f-af71-32fa53993ccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install openai package if missing\n",
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e795f8-fea2-44a5-83d2-35fb05fb12fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a99e1f4-79a3-4160-8eb0-451a1990dfdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67da741-3895-47ca-b7e2-8da1f15fc3b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_matching_score(description):\n",
    "  client = OpenAI(\n",
    "    api_key=dbutils.secrets.get(\"job_notification\", \"PAT_Token\"),\n",
    "    base_url=\"https://dbc-ecfaa4af-d4ab.cloud.databricks.com/serving-endpoints\"\n",
    "  )\n",
    "\n",
    "  chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a recruiter\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"\"\"Give the profile matching percentage in range 0 to 100, Just return the output as number and don't return any additional info \n",
    "      Ex-1 output: 10, Ex-2 output: 90\n",
    "      my details: {\n",
    "      \"Objective\": \"Data Engineer with 2+ years of experience designing and optimizing scalable ETL pipelines, data warehouses, and cloud-based solutions. Skilled in Azure, PySpark, and SQL with a proven record of improving cost efficiency, reliability, and performance. Seeking to leverage expertise in Azure Data Engineering and modern cloud platforms to build robust, data-driven solutions that support business growth.\",\n",
    "      \"Technical Skills\": \n",
    "      {\n",
    "          \"Programming Language\": \"Python, SQL, PySpark\",\n",
    "          \"Cloud & Data Services\": \"Azure Databricks, Azure Data Factory (ADF), ADLS, Azure Synapse Analytics (ASA)\",\n",
    "          \"Databases\": \"SQL Server, Oracle, Snowflake\",\n",
    "          \"Cloud Platforms\": \"Azure\",\n",
    "          \"Tools\": \"GitHub, Azure DevOps CI/CD, Prefect\",\n",
    "          \"Concepts\": \"ETL/ELT, Medallion Architecture, Data Modeling, Data Warehousing, API Integration\"\n",
    "      }\n",
    "  }\n",
    "  Job description: \"\"\" + description\n",
    "    }\n",
    "    ],\n",
    "    model=\"databricks-llama-4-maverick\",\n",
    "    max_tokens=1000\n",
    "  )\n",
    "\n",
    "  return(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48c7f843-f6e2-4012-8a48-44a69da63c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def score_job(jobs, url_col, desc_col, skill_col):\n",
    "    scored_jobs = []\n",
    "    for job in jobs:\n",
    "        description = job[desc_col]\n",
    "        if job[skill_col]:\n",
    "            description += \", \" + job[skill_col]\n",
    "        time.sleep(1)\n",
    "        score = get_matching_score(description)\n",
    "        data = {\n",
    "            \"url\": job[url_col],\n",
    "            \"score\": int(score)\n",
    "        }\n",
    "        scored_jobs.append(data)\n",
    "    return scored_jobs"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
