{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4693d185-b49a-481d-9e36-d5a9e0186c56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install openai package if missing\n",
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3cb07e7-f742-407e-8558-89a6f23d4972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b2e2303-409b-4fd1-b216-328aa8b925f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8e562e-54f2-451b-a233-23aad17f027e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_matching_score(description):\n",
    "  client = OpenAI(\n",
    "    api_key=dbutils.secrets.get(\"job_notification\", \"PAT_Token\"),\n",
    "    base_url=\"https://dbc-ecfaa4af-d4ab.cloud.databricks.com/serving-endpoints\"\n",
    "  )\n",
    "\n",
    "  chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a recruiter\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"\"\"Give the profile matching percentage in range 0 to 100, Just return the output as number and don't return any additional info \n",
    "      Ex-1 output: 10, Ex-2 output: 90\n",
    "      my details: {\n",
    "      \"Objective\": \"Data Engineer with 2+ years of experience designing and optimizing scalable ETL pipelines, data warehouses, and cloud-based solutions. Skilled in Azure, PySpark, and SQL with a proven record of improving cost efficiency, reliability, and performance. Seeking to leverage expertise in Azure Data Engineering and modern cloud platforms to build robust, data-driven solutions that support business growth.\",\n",
    "      \"Technical Skills\": \n",
    "      {\n",
    "          \"Programming Language\": \"Python, SQL, PySpark\",\n",
    "          \"Cloud & Data Services\": \"Azure Databricks, Azure Data Factory (ADF), ADLS, Azure Synapse Analytics (ASA)\",\n",
    "          \"Databases\": \"SQL Server, Oracle, Snowflake\",\n",
    "          \"Cloud Platforms\": \"Azure\",\n",
    "          \"Tools\": \"GitHub, Azure DevOps CI/CD, Prefect\",\n",
    "          \"Concepts\": \"ETL/ELT, Medallion Architecture, Data Modeling, Data Warehousing, API Integration\"\n",
    "      }\n",
    "  }\n",
    "  Job description: \"\"\" + description\n",
    "    }\n",
    "    ],\n",
    "    model=\"databricks-llama-4-maverick\",\n",
    "    max_tokens=1000\n",
    "  )\n",
    "\n",
    "  return(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ddb6e9-c113-41a7-9d0e-596380f8de1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_jobs = spark.table(\"raw_catalogue.jobs.linkedjobs\")\n",
    "jobs = all_jobs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c01281b-49d9-4ca0-961b-4c58b78954d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scored_jobs = []\n",
    "for job in jobs:\n",
    "    description = job['description']\n",
    "    if job['skills']:\n",
    "        description += \", \" + job['skills']\n",
    "    time.sleep(1)\n",
    "    score = get_matching_score(description)\n",
    "    data = {\n",
    "        \"url\": job['url'],\n",
    "        \"score\": int(score)\n",
    "    }\n",
    "    scored_jobs.append(data)\n",
    "display(scored_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3da7fe5-9556-418b-8722-06338b09aae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scored_df = spark.createDataFrame(scored_jobs)\n",
    "df = scored_df.join(all_jobs, on=[\"url\"])\n",
    "filtered_df = df.filter(df.score > 60)\n",
    "filtered_df.write.mode('overwrite').option(\"mergeSchema\", \"true\").saveAsTable(\"staging_catalogue.jobs.linkedJobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c9cd07-dd91-431f-ae89-b6b170f8489b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1772258080191}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(filtered_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "filtering linkedin jobs",
   "widgets": {
    "score_limit": {
     "currentValue": "60",
     "nuid": "8a0ac34b-aa15-4af6-a4cf-8426ac5db260",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "score_limit",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "score_limit",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
